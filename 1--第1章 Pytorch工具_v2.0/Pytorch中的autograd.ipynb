{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "关于torch.Tensor\n",
    "torch.Tensor是整个package中的核心类, 如果将属性.requires_grad设置为True, 它将追踪在这个类上定义的所有操作. 当代码要进行反向传播的时候, 直接调用.backward()就可以自动计算所有的梯度. 在这个Tensor上的所有梯度将被累加进属性.grad中.\n",
    "如果想终止一个Tensor在计算图中的追踪回溯, 只需要执行.detach()就可以将该Tensor从计算图中撤下, 在未来的回溯计算中也不会再计算该Tensor.\n",
    "除了.detach(), 如果想终止对计算图的回溯, 也就是不再进行方向传播求导数的过程, 也可以采用代码块的方式with torch.no_grad():, 这种方式非常适用于对模型进行预测的时候, 因为预测阶段不再需要对梯度进行计算."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于torch.Function:\n",
    "Function类是和Tensor类同等重要的一个核心类, 它和Tensor共同构建了一个完整的类, 每一个Tensor拥有一个.grad_fn属性, 代表引用了哪个具体的Function创建了该Tensor.\n",
    "如果某个张量Tensor是用户自定义的, 则其对应的grad_fn is None. 就比如 a = 【2，2】这种"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.ones(2, 2)  # 这个是没有开自动求导\n",
    "\n",
    "x2 = torch.ones(2, 2, requires_grad=True)  # 开启自动求导\n",
    "\n",
    "y = x1 * x2  # 这里进行计算\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(y)  # 发现这里的grad_fn是MulBackward0，也就是乘法的函数反向传播得到"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x3 = x2 + 1  # 在x2的基础上面进行一个加法操作\n",
    "print(x3)  # 就会发现这里的grad_fn是Add"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<MulBackward0 object at 0x00000219EBA723D0>\n"
     ]
    }
   ],
   "source": [
    "# grad_fn 是用来记录这个东西是不是由哪个计算的到来的，如果是用户自己定义的就是None\n",
    "print(x1.grad_fn)\n",
    "print(x2.grad_fn)\n",
    "print(y.grad_fn)  # y 是x1 和 x2 相乘得到"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于方法.requires_grad_(): 该方法可以原地改变Tensor的属性.requires_grad的值. 如果没有主动设定默认为False."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000219EBABCB20>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)  # 这里是false，因为上面的tensor没有开启自动求导\n",
    "a.requires_grad_(True)  # 这里通过魔法方法 开启了自动求导设置\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)  # 经过运算之后，就可以看到自动求导已经生效了"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于梯度Gradients\n",
    "在Pytorch中, 反向传播是依靠.backward()实现的.\n",
    "关于自动求导的属性设置: 可以通过设置.requires_grad=True来执行自动求导, 也可以通过代码块的限制来停止自动求导."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x2.requires_grad)  # 开启状态\n",
    "print((x2 ** 2).requires_grad)  # 进行计算之后也是开启状态\n",
    "\n",
    "with torch.no_grad():  # 使用关闭自动求带的块代码，一般用于预测的时候，预测就不需要起到了\n",
    "    print((x2 ** 2).requires_grad)\n",
    "print(x2.requires_grad)  # 然后上面的块运行完成之后，自动求导还是开启的，不影响住代码"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
